{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4b4b2a-a2af-4860-9ce9-4985c5ac31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bfbbfb-f266-4adb-b3ba-e5a30c41366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "\n",
    "        images_dir = os.path.join(dataset_path, split, \"images\")\n",
    "        annotations_dir = os.path.join(dataset_path, split, \"annotations\")\n",
    "\n",
    "        for filename in os.listdir(images_dir):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(images_dir, filename)\n",
    "                self.images.append(image_path)\n",
    "\n",
    "                annotation_path = os.path.join(annotations_dir, os.path.splitext(filename)[0] + \".xml\")\n",
    "                try:\n",
    "                    tree = ET.parse(annotation_path)\n",
    "                    root = tree.getroot()\n",
    "                    annotation = []\n",
    "\n",
    "                    for obj in root.findall(\"object\"):\n",
    "                        name = obj.find(\"name\").text\n",
    "                        bbox = obj.find(\"bndbox\")\n",
    "                        xmin = int(bbox.find(\"xmin\").text)\n",
    "                        ymin = int(bbox.find(\"ymin\").text)\n",
    "                        xmax = int(bbox.find(\"xmax\").text)\n",
    "                        ymax = int(bbox.find(\"ymax\").text)\n",
    "                        annotation.append((name, xmin, ymin, xmax, ymax))\n",
    "\n",
    "                    self.annotations.append(annotation)\n",
    "                except ET.ParseError:\n",
    "                    print(f\"Error parsing annotation file: {annotation_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            annotation = self.annotations[index]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            boxes = torch.as_tensor([ann[1:] for ann in annotation], dtype=torch.float32)\n",
    "            labels = torch.as_tensor([ann[0] for ann in annotation], dtype=torch.int64)\n",
    "\n",
    "            return image, {\"boxes\": boxes, \"labels\": labels}\n",
    "        except (IOError, ValueError):\n",
    "            print(f\"Error loading image file: {image_path}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fb77a3-8ced-43ec-9852-2dd8ce9ff503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for image, annotation in dataset:\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        image = transform(image)\n",
    "        \n",
    "        targets = []\n",
    "        for obj in annotation:\n",
    "            name, xmin, ymin, xmax, ymax = obj\n",
    "            label = class_to_idx[name]\n",
    "            xmin, ymin, xmax, ymax = xmin / image.shape[2], ymin / image.shape[1], xmax / image.shape[2], ymax / image.shape[1]\n",
    "            targets.append([label, xmin, ymin, xmax, ymax])\n",
    "        targets = torch.tensor(targets)\n",
    "        \n",
    "        preprocessed_images.append(image)\n",
    "        preprocessed_annotations.append(targets)\n",
    "    \n",
    "    return preprocessed_images, preprocessed_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a351344-9bb1-4856-93c0-a3eaf54885e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: {'largeCar': 0, 'heavyTruck': 1, 'lightTruck': 2, 'smallCar': 3}\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader Creation\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  \n",
    "    transforms.RandomHorizontalFlip(0.5),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "dataset = CustomDataset(dataset_path, split, transform)\n",
    "\n",
    "class_labels = set()\n",
    "for _, annotation in dataset:\n",
    "    for obj in annotation:\n",
    "        name = obj[0]\n",
    "        class_labels.add(name)\n",
    "\n",
    "class_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "preprocessed_images, preprocessed_annotations = preprocess_dataset(dataset)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = list(zip(preprocessed_images, preprocessed_annotations))\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62116b6a-7c4f-4361-b539-0f1d531c7386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4418\n",
      "Epoch [2/10], Loss: 0.3149\n",
      "Epoch [3/10], Loss: 0.2285\n",
      "Epoch [4/10], Loss: 0.3032\n",
      "Epoch [5/10], Loss: 0.2469\n",
      "Epoch [6/10], Loss: 0.1760\n",
      "Epoch [7/10], Loss: 0.2289\n",
      "Epoch [8/10], Loss: 0.2086\n",
      "Epoch [9/10], Loss: 0.1681\n",
      "Epoch [10/10], Loss: 0.1353\n"
     ]
    }
   ],
   "source": [
    "# Model Definition and Training\n",
    "num_classes = len(class_to_idx) + 1\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{'boxes': t[:, 1:], 'labels': t[:, 0].long()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += losses.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce10856c-392b-4009-b458-7c4553fd80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Testing\n",
    "model.eval()\n",
    "\n",
    "test_dataset = CustomDataset(dataset_path, split=\"test\", transform=transform)\n",
    "test_images, test_annotations = preprocess_dataset(test_dataset)\n",
    "test_dataset = list(zip(test_images, test_annotations))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for image, target in test_loader:\n",
    "    image = image[0].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    confidence_threshold = 0.5\n",
    "    mask = scores >= confidence_threshold\n",
    "    boxes = boxes[mask]\n",
    "    labels = labels[mask]\n",
    "    scores = scores[mask]\n",
    "    \n",
    "    image = image.squeeze(0).cpu().numpy().transpose((1, 2, 0))  # Remove batch dimension and change shape to [H, W, C]\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        label_name = list(class_to_idx.keys())[list(class_to_idx.values()).index(label)]\n",
    "        \n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Object Detection\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd00703-0256-43c0-b7e0-dfd12bda7381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Video Object Detection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m input_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridge_1.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m input_video \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(input_video_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Video Object Detection\n",
    "model.eval()\n",
    "\n",
    "input_video_path = \"bridge_1.mp4\"\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    image = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image)\n",
    "\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    confidence_threshold = 0.5\n",
    "    mask = scores >= confidence_threshold\n",
    "    boxes = boxes[mask]\n",
    "    labels = labels[mask]\n",
    "    scores = scores[mask]\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        label_name = list(class_to_idx.keys())[list(class_to_idx.values()).index(label)]\n",
    "\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "input_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ca309-781e-4ea9-8194-c8348a9f6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_video_into_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return fps\n",
    "\n",
    "input_video_path = \"\"\n",
    "\n",
    "output_frames_dir = \"\"\n",
    "\n",
    "fps = slice_video_into_frames(input_video_path, output_frames_dir)\n",
    "\n",
    "print(f\"Video sliced into frames at {fps} FPS. Frames saved in {output_frames_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11042594-fc1f-428a-a464-70a7445127ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     22\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     23\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m800\u001b[39m)),\n\u001b[1;32m     24\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     25\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     26\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame_path, transform):\n",
    "    frame = Image.open(frame_path).convert(\"RGB\")\n",
    "    preprocessed_frame = transform(frame)\n",
    "    return preprocessed_frame\n",
    "\n",
    "def detect_objects(model, frame_path, transform, device):\n",
    "    preprocessed_frame = preprocess_frame(frame_path, transform)\n",
    "    input_tensor = preprocessed_frame.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "frames_dir = \"\"\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for frame_name in os.listdir(frames_dir):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    predictions = detect_objects(model, frame_path, transform, device)\n",
    "    print(f\"Processed frame: {frame_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0792e354-99dd-493e-809f-214b7d877b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m vehicle_counts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m vehicle_locations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(frames_dir):\n\u001b[1;32m     20\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frames_dir, frame_name)\n\u001b[1;32m     21\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m detect_objects(model, frame_path, transform, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def count_vehicles(predictions, confidence_threshold=0.5):\n",
    "    vehicle_count = 0\n",
    "    vehicle_boxes = []\n",
    "\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score >= confidence_threshold and label == 1:  \n",
    "            vehicle_count += 1\n",
    "            vehicle_boxes.append(box)\n",
    "\n",
    "    return vehicle_count, vehicle_boxes\n",
    "\n",
    "vehicle_counts = []\n",
    "vehicle_locations = []\n",
    "\n",
    "for frame_name in os.listdir(frames_dir):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    predictions = detect_objects(model, frame_path, transform, device)\n",
    "\n",
    "    count, boxes = count_vehicles(predictions, confidence_threshold=0.5)\n",
    "    vehicle_counts.append(count)\n",
    "    vehicle_locations.append(boxes)\n",
    "\n",
    "    print(f\"Processed frame: {frame_name}, Vehicle count: {count}\")\n",
    "\n",
    "total_vehicles = sum(vehicle_counts)\n",
    "print(f\"Total vehicles detected: {total_vehicles}\")\n",
    "print(\"Vehicle locations:\")\n",
    "for i, (count, boxes) in enumerate(zip(vehicle_counts, vehicle_locations)):\n",
    "    print(f\"Frame {i}: Count: {count}, Boxes: {boxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f84a5a2-73cb-4842-804d-b900bdeb8667",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vehicle_images\n\u001b[1;32m     14\u001b[0m vehicle_images_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(vehicle_images_dir):\n\u001b[1;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(vehicle_images_dir)\n\u001b[1;32m     19\u001b[0m vehicle_image_paths \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_vehicle_images(frame_path, vehicle_boxes, output_dir):\n",
    "    frame = cv2.imread(frame_path)\n",
    "    vehicle_images = []\n",
    "\n",
    "    for i, box in enumerate(vehicle_boxes):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        vehicle_image = frame[ymin:ymax, xmin:xmax]\n",
    "        vehicle_image_path = os.path.join(output_dir, f\"vehicle_{i}.jpg\")\n",
    "        cv2.imwrite(vehicle_image_path, vehicle_image)\n",
    "        vehicle_images.append(vehicle_image_path)\n",
    "\n",
    "    return vehicle_images\n",
    "\n",
    "vehicle_images_dir = \"\"\n",
    "\n",
    "if not os.path.exists(vehicle_images_dir):\n",
    "    os.makedirs(vehicle_images_dir)\n",
    "\n",
    "vehicle_image_paths = []\n",
    "\n",
    "for frame_name, boxes in zip(os.listdir(frames_dir), vehicle_locations):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    vehicle_images = extract_vehicle_images(frame_path, boxes, vehicle_images_dir)\n",
    "    vehicle_image_paths.extend(vehicle_images)\n",
    "\n",
    "    print(f\"Extracted vehicle images for frame: {frame_name}\")\n",
    "\n",
    "print(\"Extracted vehicle images:\")\n",
    "for path in vehicle_image_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d30416-5988-4880-9795-41b975c41d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m         _, predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_class\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 16\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Assuming 4 vehicle classes: car, truck, bus, motorcycle\u001b[39;00m\n\u001b[1;32m     18\u001b[0m num_features \u001b[38;5;241m=\u001b[39m resnet_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_vehicle_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = transform(image)\n",
    "    return preprocessed_image\n",
    "\n",
    "def classify_vehicle(model, image_path, transform, device):\n",
    "    preprocessed_image = preprocess_vehicle_image(image_path, transform)\n",
    "    input_tensor = preprocessed_image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "num_classes = 4  \n",
    "num_features = resnet_model.fc.in_features\n",
    "resnet_model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vehicle_classifications = []\n",
    "\n",
    "for image_path in vehicle_image_paths:\n",
    "    predicted_class = classify_vehicle(resnet_model, image_path, resnet_transform, device)\n",
    "    vehicle_classifications.append(predicted_class)\n",
    "\n",
    "    print(f\"Classified vehicle: {image_path}, Class: {predicted_class}\")\n",
    "\n",
    "print(\"Vehicle classifications using ResNet:\")\n",
    "for image_path, classification in zip(vehicle_image_paths, vehicle_classifications):\n",
    "    print(f\"Image: {image_path}, Class: {classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933baca1-8f09-484e-b026-fa68ef4be801",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Precision, Recall, F1Score, Accuracy\n\u001b[1;32m      3\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlargeCar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmallCar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheavyTruck\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightTruck\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [class_labels[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vehicle_classifications]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall, F1Score, Accuracy\n",
    "\n",
    "class_labels = {\n",
    "    0: 'largeCar',\n",
    "    1: 'smallCar',\n",
    "    2: 'heavyTruck',\n",
    "    3: 'lightTruck'\n",
    "}\n",
    "\n",
    "predicted_labels = [class_labels[cls] for cls in vehicle_classifications]\n",
    "\n",
    "ground_truth_labels = [\n",
    "    'largeCar',\n",
    "    'smallCar',\n",
    "    'heavyTruck',\n",
    "    'lightTruck',\n",
    "    'smallCar',\n",
    "    'largeCar',\n",
    "    'heavyTruck',\n",
    "    'largeCar',\n",
    "    'lightTruck',\n",
    "    'smallCar'\n",
    "]\n",
    "\n",
    "ground_truth_indices = [list(class_labels.values()).index(label) for label in ground_truth_labels]\n",
    "\n",
    "predicted_labels_tensor = torch.tensor(vehicle_classifications)\n",
    "ground_truth_indices_tensor = torch.tensor(ground_truth_indices)\n",
    "\n",
    "precision = Precision(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "recall = Recall(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "f1_score = F1Score(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "accuracy = Accuracy()(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "\n",
    "print(\"Vehicle Classification Evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574071eb-277c-4dda-84f4-edb645b2b94e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m processed_frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m labeled_frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(labeled_frames_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_name, boxes, labels, scores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(frames_dir), vehicle_locations, vehicle_classifications, vehicle_scores):\n\u001b[1;32m     19\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frames_dir, frame_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def draw_bounding_boxes(image, boxes, labels, scores, class_labels, confidence_threshold=0.5):\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= confidence_threshold:\n",
    "            xmin, ymin, xmax, ymax = box.astype(int)\n",
    "            class_name = class_labels[label]\n",
    "            \n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{class_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "processed_frames_dir = \"\"\n",
    "\n",
    "labeled_frames_dir = \"\"\n",
    "os.makedirs(labeled_frames_dir, exist_ok=True)\n",
    "\n",
    "for frame_name, boxes, labels, scores in zip(os.listdir(frames_dir), vehicle_locations, vehicle_classifications, vehicle_scores):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    labeled_frame = draw_bounding_boxes(frame, boxes, labels, scores, class_labels)\n",
    "    \n",
    "    labeled_frame_path = os.path.join(labeled_frames_dir, frame_name)\n",
    "    cv2.imwrite(labeled_frame_path, labeled_frame)\n",
    "    \n",
    "    print(f\"Labeled frame: {frame_name}\")\n",
    "\n",
    "print(\"Labeling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d189-6b96-4c0b-9ca0-1a9638635858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_video_from_frames(frames_dir, output_path, fps):\n",
    "    frame_files = sorted(os.listdir(frames_dir), key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    \n",
    "    if len(frame_files) == 0:\n",
    "        print(\"No frames found in the directory.\")\n",
    "        return\n",
    "    \n",
    "    first_frame_path = os.path.join(frames_dir, frame_files[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    height, width, _ = first_frame.shape\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    video_writer.release()\n",
    "    \n",
    "    print(\"Video reconstruction completed.\")\n",
    "\n",
    "labeled_frames_dir = \"\"\n",
    "\n",
    "output_video_path = \"\"\n",
    "\n",
    "output_fps = 30\n",
    "\n",
    "rebuild_video_from_frames(labeled_frames_dir, output_video_path, output_fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
