{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4b4b2a-a2af-4860-9ce9-4985c5ac31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb1b679-61b8-41eb-8336-df6c5d493add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"  \n",
    "dataset = CustomDataset(dataset_path, split)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        images_dir = os.path.join(dataset_path, split, \"images\")\n",
    "        annotations_dir = os.path.join(dataset_path, split, \"annotations\")\n",
    "\n",
    "        for filename in os.listdir(images_dir):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(images_dir, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                self.images.append(image)\n",
    "\n",
    "                annotation_path = os.path.join(annotations_dir, filename.split(\".\")[0] + \".xml\")\n",
    "                tree = ET.parse(annotation_path)\n",
    "                root = tree.getroot()\n",
    "                annotation = []\n",
    "                for obj in root.findall(\"object\"):\n",
    "                    name = obj.find(\"name\").text\n",
    "                    bbox = obj.find(\"bndbox\")\n",
    "                    xmin = int(bbox.find(\"xmin\").text)\n",
    "                    ymin = int(bbox.find(\"ymin\").text)\n",
    "                    xmax = int(bbox.find(\"xmax\").text)\n",
    "                    ymax = int(bbox.find(\"ymax\").text)\n",
    "                    annotation.append((name, xmin, ymin, xmax, ymax))\n",
    "                self.annotations.append(annotation)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        annotation = self.annotations[index]\n",
    "        return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33cbfc38-9b55-4072-a3cc-f1af8c9473c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: {'heavyTruck': 0, 'lightTruck': 1, 'smallCar': 2, 'largeCar': 3}\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((800, 800)),  \n",
    "    transforms.RandomHorizontalFlip(0.5),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "class_labels = set()\n",
    "for _, annotation in dataset:\n",
    "    for obj in annotation:\n",
    "        name = obj[0]\n",
    "        class_labels.add(name)\n",
    "\n",
    "class_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for image, annotation in dataset:\n",
    "        image = transform(image)\n",
    "        \n",
    "        targets = []\n",
    "        for obj in annotation:\n",
    "            name, xmin, ymin, xmax, ymax = obj\n",
    "            label = class_to_idx[name]\n",
    "            xmin, ymin, xmax, ymax = xmin / image.shape[2], ymin / image.shape[1], xmax / image.shape[2], ymax / image.shape[1]\n",
    "            targets.append([label, xmin, ymin, xmax, ymax])\n",
    "        targets = torch.tensor(targets)\n",
    "        \n",
    "        preprocessed_images.append(image)\n",
    "        preprocessed_annotations.append(targets)\n",
    "    \n",
    "    return preprocessed_images, preprocessed_annotations\n",
    "\n",
    "preprocessed_images, preprocessed_annotations = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8280789-72f1-479a-8eaa-4ef37baeb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "def get_fasterrcnn_model(num_classes):\n",
    "    backbone = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\n",
    "    backbone.out_channels = 2048\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    model = FasterRCNN(backbone,\n",
    "                       num_classes=num_classes,\n",
    "                       rpn_anchor_generator=anchor_generator,\n",
    "                       box_roi_pool=roi_pooler)\n",
    "    return model\n",
    "\n",
    "num_classes = len(class_to_idx) + 1\n",
    "model = get_fasterrcnn_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2451d5ee-2e58-40cc-ac55-05d77ac7f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        \n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        images_dir = os.path.join(self.dataset_path, self.split, \"images\")\n",
    "        annotations_dir = os.path.join(self.dataset_path, self.split, \"annotations\")\n",
    "        \n",
    "        for filename in os.listdir(images_dir):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(images_dir, filename)\n",
    "                annotation_path = os.path.join(annotations_dir, filename.split(\".\")[0] + \".xml\")\n",
    "                \n",
    "                self.images.append(image_path)\n",
    "                self.annotations.append(annotation_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        annotation_path = self.annotations[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = int(bbox.find(\"xmin\").text)\n",
    "            ymin = int(bbox.find(\"ymin\").text)\n",
    "            xmax = int(bbox.find(\"xmax\").text)\n",
    "            ymax = int(bbox.find(\"ymax\").text)\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfbbfb-f266-4adb-b3ba-e5a30c41366b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
